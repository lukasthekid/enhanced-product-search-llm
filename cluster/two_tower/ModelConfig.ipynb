{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d73a43-85ca-4d1f-9d95-0813c31ffd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:17:26.086384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-11 16:17:26.086426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-11 16:17:26.088088: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 16:17:26.096939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-11 16:17:29.660920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/e11925939/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 16:17:46 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  |   00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             25W /  250W |       1MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           On  |   00000000:B5:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             27W /  250W |       1MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "TensorFlow version: 2.15.0\n",
      "Num GPUs Available:  2\n",
      "PyTorch version: 2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "SEED = 11925939\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "!nvidia-smi\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1344ba1-80ed-44fa-9064-973406d2b9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip uninstall --y tf_keras tensorflow tensorflow-recommenders\n",
    "#%pip install tensorflow==2.15.0\n",
    "#%pip install tensorflow_recommenders\n",
    "#%pip install tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f922a735-4a58-4a5d-85bf-32ae19555af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "    \"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"Model for encoding user queries.\n",
    "\n",
    "        Args:\n",
    "          layers:\n",
    "            A list of Layer configuration objects where the i-th entry represents the number of units\n",
    "            and the activation function the i-th layer contains.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Append deep Layers\n",
    "        for layer in layers:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer[\"size\"], activation=layer[\"act_fn\"]))\n",
    "\n",
    "        # Normalize The Output\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=-1)))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense_layers(inputs)\n",
    "\n",
    "\n",
    "class ProductModel(tf.keras.Model):\n",
    "    \"\"\"Model for encoding products.\"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"Model for encoding products.\n",
    "\n",
    "        Args:\n",
    "          layers:\n",
    "            A list of Layer configuration objects where the i-th entry represents the number of units\n",
    "            and the activation function the i-th layer contains.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Then construct the layers.\n",
    "        self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "        # Append deep Layers\n",
    "        for layer in layers:\n",
    "            self.dense_layers.add(tf.keras.layers.Dense(layer[\"size\"], activation=layer[\"act_fn\"]))\n",
    "\n",
    "        self.dense_layers.add(tf.keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=-1)))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense_layers(inputs)\n",
    "\n",
    "\n",
    "class TwoTowerModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, model_config, candidates):\n",
    "        super().__init__()\n",
    "        self.query_model = QueryModel(model_config)\n",
    "        self.product_model = ProductModel(model_config)\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=candidates.batch(128).map(self.query_model)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        query_embeddings = self.query_model(features[\"query_embedding\"])\n",
    "        product_embeddings = self.product_model(features[\"product_embedding\"])\n",
    "\n",
    "        return self.task(\n",
    "            product_embeddings, query_embeddings, compute_metrics=not training)\n",
    "\n",
    "\n",
    "def loading_data(pairs: pd.DataFrame) -> (tf.data.Dataset, tf.data.Dataset):\n",
    "    pairs['product_embedding'] = pairs['product_embedding'].apply(np.array)\n",
    "    pairs['query_embedding'] = pairs['query_embedding'].apply(np.array)\n",
    "    unique_embeddings = pairs.groupby('id')['product_embedding'].first().values\n",
    "    \n",
    "    print('Compete Dataset Shape:', pairs.shape, 'unique candidates: {}', len(unique_embeddings))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices({\n",
    "        \"query_embedding\": np.stack(pairs['query_embedding'].values),\n",
    "        \"product_embedding\": np.stack(pairs['product_embedding'].values)\n",
    "    })\n",
    "    shuffled = dataset.shuffle(len(pairs), seed=SEED, reshuffle_each_iteration=False)\n",
    "    candidates_dataset = tf.data.Dataset.from_tensor_slices(np.stack(unique_embeddings))\n",
    "    del pairs\n",
    "    return shuffled, candidates_dataset\n",
    "\n",
    "\n",
    "def find_best_config(num_epochs: int, configs, cached_train, cached_test, candidates_dataset):\n",
    "    best_accuracy = {'config': None, 'accuracy': -1}\n",
    "\n",
    "    for i in range(len(configs)):\n",
    "        torch.cuda.empty_cache()\n",
    "        model = TwoTowerModel(configs[i], candidates_dataset)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "        history = model.fit(\n",
    "            cached_train,\n",
    "            validation_data=cached_test,\n",
    "            epochs=num_epochs,\n",
    "            verbose=0)\n",
    "\n",
    "        accuracy = history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "        if accuracy > best_accuracy['accuracy']:\n",
    "            best_accuracy = {'config': configs[i], 'accuracy': accuracy}\n",
    "\n",
    "    return best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49dc8c0d-606e-418f-91e0-1873bcb06921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compete Dataset Shape: (100000, 9) unique candidates: {} 90799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:20:21.946790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31134 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n",
      "2024-12-11 16:20:21.947568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31134 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b5:00.0, compute capability: 7.0\n",
      "2024-12-11 16:20:24.043777: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs that are brute forced [[{'size': 128, 'act_fn': 'elu'}, {'size': 128, 'act_fn': None}], [{'size': 256, 'act_fn': 'elu'}, {'size': 128, 'act_fn': 'elu'}, {'size': 128, 'act_fn': None}], [{'size': 512, 'act_fn': 'elu'}, {'size': 256, 'act_fn': 'elu'}, {'size': 128, 'act_fn': 'elu'}, {'size': 128, 'act_fn': None}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:20:29.136091: I external/local_xla/xla/service/service.cc:168] XLA service 0x1517218c5830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-11 16:20:29.136133: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2024-12-11 16:20:29.136183: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2024-12-11 16:20:29.141797: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-11 16:20:29.188824: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733930429.226959 1722958 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution {'config': [{'size': 128, 'act_fn': 'elu'}, {'size': 128, 'act_fn': None}], 'accuracy': 0.20145000517368317}\n",
      "time passed 418.06436586380005 seconds\n"
     ]
    }
   ],
   "source": [
    "pairs = pd.read_parquet('../synth_set/synthetic_positive_pairs.parquet.gzip').sample(n=100_000,random_state=SEED)\n",
    "pairs = pd.merge(pairs, pd.read_pickle('../p_collection.pkl'), on='id')\n",
    "pairs.dropna(subset=['query', 'product_embedding'], inplace=True)\n",
    "\n",
    "l:int = len(pairs)\n",
    "\n",
    "shuffled, candidates_dataset = loading_data(pairs)\n",
    "train = shuffled.take(int(l*0.8))\n",
    "test = shuffled.skip(int(l*0.8)).take(int(l*0.2))\n",
    "\n",
    "batch_size = 2048\n",
    "cached_train = train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "cached_test = test.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "configs = []\n",
    "k = 0\n",
    "for i in range(3):\n",
    "    config = []\n",
    "    config.append({'size': 128, 'act_fn': None})\n",
    "    config.append({'size': 128, 'act_fn': 'elu'})\n",
    "    for j in range(k):\n",
    "        size = int(config[len(config) - 1]['size'])\n",
    "        config.append({'size': size * 2, 'act_fn': 'elu'})\n",
    "    k += 1\n",
    "    configs.append(config[::-1])\n",
    "\n",
    "print(\"Configs that are brute forced\", configs)\n",
    "start = time.time()\n",
    "config = find_best_config(5, configs, cached_train, cached_test, candidates_dataset)\n",
    "print('solution', config)\n",
    "print('time passed', (time.time() - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391834b4-ad6d-4545-9de3-6d94524f34c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
