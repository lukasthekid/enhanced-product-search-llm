{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952e8c9d-5acc-4f22-a1d7-245d4bfd8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U tensorflow[and-cuda] tensorflow_recommenders sentence_transformers tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149e479f-450b-46be-84d2-0aeecac22ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 14:15:09.492489: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-13 14:15:09.514013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739452509.529267 1657621 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739452509.534086 1657621 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 14:15:09.551595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPU(s) Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "TensorFlow version: 2.18.0\n",
      "2.4.0+cu121\n",
      "True\n",
      "12.1\n",
      "Generating 980974 embeddings took 911.9814088344574 seconds\n",
      "NDCG@10: 0.5841745161290323\n",
      "Precision@5: 0.6473118279569893\n",
      "Recall@5: 0.16199503960159423\n",
      "\n",
      "\n",
      "NDCG@10: 0.5841745161290323\n",
      "Precision@10: 0.5763440860215053\n",
      "Recall@10: 0.24305364129403814\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pyarrow.parquet as pq\n",
    "from typing import Tuple, Generator, Dict, Text\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 11925939\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "GENRATE_EMBEDDINGS = True\n",
    "\n",
    "# Configure TensorFlow for optimal GPU usage\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # If using PyTorch elsewhere\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"{len(gpus)} GPU(s) Available: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs found. Using CPU.\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "PCOLLECTION_FILE_PATH = '../p_collection.pkl'\n",
    "ALL_MINI_L6_EMBEDDINGS = 'all-MiniLM-L6-v2.pkl'\n",
    "QUERY_TO_QUERY = '../qid2query.tsv'\n",
    "QREL_TEST = '../QREL/2023test.qrel'\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)           # PyTorch version\n",
    "print(torch.cuda.is_available())    # Should be True\n",
    "print(torch.version.cuda)  \n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Remove PYTORCH_CUDA_ALLOC_CONF from the environment if it exists\n",
    "os.environ.pop(\"PYTORCH_CUDA_ALLOC_CONF\", None)\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return sentence_transformer.encode(batch)\n",
    "\n",
    "\n",
    "sentence_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "collection = pd.read_pickle(PCOLLECTION_FILE_PATH)\n",
    "\n",
    "if GENRATE_EMBEDDINGS:\n",
    "    start = time.time()\n",
    "    product_texts = collection['product_text'].values\n",
    "    embeddings = np.stack(sentence_transformer.encode(product_texts))\n",
    "\n",
    "    # Create a DataFrame with the embeddings\n",
    "    df = pd.DataFrame(index=range(len(embeddings)))  # Initialize with correct number of rows\n",
    "    # Store embeddings as list of arrays\n",
    "    df['product_embeddings'] = [emb for emb in embeddings]\n",
    "    df.to_pickle(ALL_MINI_L6_EMBEDDINGS)\n",
    "    print(f'Generating {len(df)} embeddings took {time.time() - start} seconds')\n",
    "    del df\n",
    "    \n",
    "\n",
    "\n",
    "def produce_ground_truth(qid: int, qrel: pd.DataFrame, collection: pd.DataFrame) -> [int]:\n",
    "    df = qrel[(qrel['qid'] == qid) & (qrel['docid'].isin(collection['id'].values))]\n",
    "    df = df.sort_values(by='relevance_score', ascending=False)\n",
    "    return df.set_index('docid')['relevance_score'].to_dict()\n",
    "\n",
    "def produce_y_pred(topk: pd.DataFrame, y_true) -> dict:\n",
    "    # Create a result dictionary or DataFrame if needed\n",
    "    matched_scores = [y_true[_id] for _id in topk['id'].values]\n",
    "    result = {'docid': topk['id'].values, 'relevance_score': matched_scores}\n",
    "    return pd.DataFrame(result).set_index('docid')['relevance_score'].to_dict()\n",
    "        \n",
    "def normalized_discounted_cumulative_gain(temp_set, p=10):\n",
    "    dc_gain = 0\n",
    "    idc_gain = 0\n",
    "    for idx, value in enumerate(temp_set.values()):\n",
    "        pos = idx + 1\n",
    "        dc_gain += value / math.log2(pos + 1)\n",
    "        if pos == p:\n",
    "            break\n",
    "    for idx, value in enumerate(sorted(temp_set.values(), reverse=True)):\n",
    "        pos = idx + 1\n",
    "        idc_gain += value / math.log2(pos + 1)\n",
    "        if pos == p:\n",
    "            break\n",
    "    return round(dc_gain / idc_gain, 5)\n",
    "\n",
    "def precision_at_k(predicted_dict, ideal_dict, k):\n",
    "    # Get the top K docids from the predicted results\n",
    "    top_k_pred = list(predicted_dict.keys())[:k]\n",
    "    # Count the number of relevant documents in the top K predicted results\n",
    "    relevant_in_pred = sum([1 for docid in top_k_pred if ideal_dict.get(docid, 0) > 0])\n",
    "    # Precision is the number of relevant documents divided by K\n",
    "    return relevant_in_pred / k\n",
    "\n",
    "\n",
    "def recall_at_k(predicted_dict, ideal_dict, k):\n",
    "    # Get the top K docids from the predicted results\n",
    "    top_k_pred = list(predicted_dict.keys())[:k]\n",
    "    # Count the total number of relevant documents in the ideal results\n",
    "    total_relevant = sum([1 for score in ideal_dict.values() if score > 0])\n",
    "    # Count the number of relevant documents in the top K predicted results\n",
    "    relevant_in_pred = sum([1 for docid in top_k_pred if ideal_dict.get(docid, 0) > 0])\n",
    "    # Recall is the number of relevant documents in top K divided by the total number of relevant documents\n",
    "    return relevant_in_pred / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def evaluate_two_tower(query_df: pd.DataFrame, qrel_df: pd.DataFrame, model,\n",
    "                       collection: pd.DataFrame, sentence_transformer: SentenceTransformer, k=10 ) -> (\n",
    "        float, float, float):\n",
    "    ndcg = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for _, row in query_df.iterrows():\n",
    "        if row.text is np.nan:\n",
    "            continue\n",
    "        #optimal ranking\n",
    "        y_true = produce_ground_truth(int(row.qid), qrel_df, collection)\n",
    "        # get the candidates\n",
    "        df_candidates = collection[collection['id'].isin(y_true.keys())].copy()\n",
    "        candidates = tf.data.Dataset.from_tensor_slices(np.stack(df_candidates['product_embedding'].values))\n",
    "        # build index\n",
    "        brute_force = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "        brute_force.index_from_dataset(candidates.batch(128).map(model.product_model))\n",
    "        # predict top k\n",
    "        scores, indices = brute_force(np.array(sentence_transformer.encode([str(row.text)])), k=df_candidates.shape[0])\n",
    "        indices = indices.numpy().flatten()\n",
    "        scores = scores.numpy().flatten()\n",
    "        topk_df = df_candidates.iloc[indices].copy()\n",
    "        topk_df['score'] = scores\n",
    "        topk_df = topk_df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "        y_pred = produce_y_pred(topk_df, y_true)\n",
    "        ndcg.append(normalized_discounted_cumulative_gain(y_pred))\n",
    "        precision.append(precision_at_k(y_pred, y_true, k))\n",
    "        recall.append(recall_at_k(y_pred, y_true, k))\n",
    "\n",
    "    return np.mean(ndcg), np.mean(precision), np.mean(recall)\n",
    "\n",
    "\n",
    "def evaluate_gte(query_df: pd.DataFrame, qrel_df: pd.DataFrame,\n",
    "                       collection: pd.DataFrame, sentence_transformer: SentenceTransformer, k=10) -> (\n",
    "        float, float, float):\n",
    "    ndcg = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for _, row in query_df.iterrows():\n",
    "        if row.text is np.nan:\n",
    "            continue\n",
    "        #optimal ranking\n",
    "        y_true = produce_ground_truth(int(row.qid), qrel_df, collection)\n",
    "        # get the candidates\n",
    "        df_candidates = collection[collection['id'].isin(y_true.keys())].copy()\n",
    "        product_embeddings = np.stack(df_candidates['product_embedding'].values)\n",
    "        query_embedding = np.array(sentence_transformer.encode([str(row.text)]))\n",
    "        df_candidates['score'] = cosine_similarity(query_embedding, product_embeddings).flatten()\n",
    "        topk_df = df_candidates.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "        y_pred = produce_y_pred(topk_df, y_true)\n",
    "        ndcg.append(normalized_discounted_cumulative_gain(y_pred))\n",
    "        precision.append(precision_at_k(y_pred, y_true, k))\n",
    "        recall.append(recall_at_k(y_pred, y_true, k))\n",
    "\n",
    "    return np.mean(ndcg), np.mean(precision), np.mean(recall)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = pd.read_pickle(ALL_MINI_L6_EMBEDDINGS)\n",
    "collection['product_embedding'] = embeddings['product_embeddings']\n",
    "qid2query_df = pd.read_csv(QUERY_TO_QUERY, sep='\\t', names=['qid', 'text'], header=None)\n",
    "qrel_df = pd.read_csv(QREL_TEST, sep='\\t', names=['qid', '0', 'docid', 'relevance_score'], header=None)\n",
    "common_qids = set(qrel_df['qid']).intersection(set(qid2query_df['qid']))\n",
    "qrel_df = qrel_df[qrel_df['qid'].isin(common_qids)]\n",
    "qid2query_df = qid2query_df[qid2query_df['qid'].isin(common_qids)]\n",
    "\n",
    "k = 5\n",
    "score = evaluate_gte(qid2query_df, qrel_df, collection, sentence_transformer, k=k)\n",
    "print(f'NDCG@10: {score[0]}')\n",
    "print(f\"Precision@{k}: {score[1]}\")\n",
    "print(f\"Recall@{k}: {score[2]}\")\n",
    "print('\\n')\n",
    "k = 10\n",
    "score = evaluate_gte(qid2query_df, qrel_df, collection, sentence_transformer, k=k)\n",
    "print(f'NDCG@10: {score[0]}')\n",
    "print(f\"Precision@{k}: {score[1]}\")\n",
    "print(f\"Recall@{k}: {score[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3beca-3264-443b-80c0-f020e8d78d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
